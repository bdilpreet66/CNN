{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMZZXFJ6X1q/MGaNMo0QiGo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bdilpreet66/CNN/blob/main/COCO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KWnDjeb4tVt",
        "outputId": "65c57da3-2b62-46eb-fd7a-6fe9fab52136"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (2.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pycocotools) (1.22.4)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools) (3.7.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (23.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (4.39.3)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (8.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pycocotools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://images.cocodataset.org/zips/train2017.zip\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEOXjfG-5RBq",
        "outputId": "93bd13fe-93ac-41b9-d91f-34eeadb61e94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-28 03:25:56--  http://images.cocodataset.org/zips/train2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 3.5.25.205, 3.5.9.11, 54.231.169.89, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|3.5.25.205|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19336861798 (18G) [application/zip]\n",
            "Saving to: ‘train2017.zip’\n",
            "\n",
            "train2017.zip        96%[==================> ]  17.44G  77.7MB/s    eta 8s     "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip train2017.zip -d train2017\n",
        "!unzip annotations_trainval2017.zip -d annotations"
      ],
      "metadata": {
        "id": "gD_z8DRW5V8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pycocotools.coco import COCO\n",
        "\n",
        "# Initialize the COCO API for instance annotations\n",
        "coco = COCO('annotations/instances_train2017.json')\n",
        "\n",
        "# Load the categories in the dataset\n",
        "categories = coco.loadCats(coco.getCatIds())\n",
        "\n",
        "# Load the annotations for the dataset\n",
        "annotations = coco.loadAnns(coco.getAnnIds())"
      ],
      "metadata": {
        "id": "I4Kprv0i5fo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "fEVOj9K45jT6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the input shape and number of classes for the NiN model\n",
        "input_shape = (None, None, 3)\n",
        "num_classes = 80\n",
        "\n",
        "# Define the NiN model\n",
        "def nin_model(input_shape, num_classes):\n",
        "    model = tf.keras.Sequential([\n",
        "        # Convolutional layer 1 with 192 filters and 5x5 kernel\n",
        "        tf.keras.layers.Conv2D(filters=192, kernel_size=5, padding='same', activation='relu', input_shape=input_shape),\n",
        "        # NiN block 1 with 192 filters and 1x1 kernel\n",
        "        tf.keras.layers.Conv2D(filters=160, kernel_size=1, padding='same', activation='relu'),\n",
        "        tf.keras.layers.Conv2D(filters=96, kernel_size=1, padding='same', activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(pool_size=3, strides=2),\n",
        "        # NiN block 2 with 192 filters and 1x1 kernel\n",
        "        tf.keras.layers.Conv2D(filters=192, kernel_size=5, padding='same', activation='relu'),\n",
        "        tf.keras.layers.Conv2D(filters=192, kernel_size=1, padding='same', activation='relu'),\n",
        "        tf.keras.layers.Conv2D(filters=192, kernel_size=1, padding='same', activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(pool_size=3, strides=2),\n",
        "        # NiN block 3 with 192 filters and 1x1 kernel\n",
        "        tf.keras.layers.Conv2D(filters=192, kernel_size=3, padding='same', activation='relu'),\n",
        "        tf.keras.layers.Conv2D(filters=192, kernel_size=1, padding='same', activation='relu'),\n",
        "        tf.keras.layers.Conv2D(filters=num_classes, kernel_size=1, padding='same', activation='softmax'),\n",
        "        # Global average pooling\n",
        "        tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Create the NiN model\n",
        "model = nin_model(input_shape, num_classes)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.1), loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14vdRJ959yK3",
        "outputId": "7a6ad565-4f2d-4ade-c528-b08aff8d0cee"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function to read the TFRecord files and parse the data\n",
        "def parse_tfrecord(serialized_example):\n",
        "    feature_description = {\n",
        "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
        "        'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32),\n",
        "        'image/object/bbox/ymin': tf.io.VarLenFeature(tf.float32),\n",
        "        'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),\n",
        "        'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32),\n",
        "        'image/object/class/label': tf.io.VarLenFeature(tf.int64),\n",
        "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
        "    }\n",
        "    example = tf.io.parse_single_example(serialized_example, feature_description)\n",
        "    image = tf.image.decode_jpeg(example['image/encoded'], channels=3)\n",
        "    height = tf.cast(example['image/height'], tf.float32)\n",
        "    width = tf.cast(example['image/width'], tf.float32)\n",
        "    xmin = tf.sparse.to_dense(example['image/object/bbox/xmin'])\n",
        "    ymin = tf.sparse.to_dense(example['image/object/bbox/ymin'])\n",
        "    xmax = tf.sparse.to_dense(example['image/object/bbox/xmax'])\n",
        "    ymax = tf.sparse.to_dense(example['image/object/bbox/ymax'])\n",
        "    label = tf.sparse.to_dense(example['image/object/class/label'])\n",
        "    # Normalize the image\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    # Concatenate the bounding boxes and labels into a single tensor\n",
        "    boxes = tf.stack([ymin, xmin, ymax, xmax], axis=-1)\n",
        "    labels = tf.one_hot(label - 1, num_classes)\n",
        "    return image, boxes, labels, height, width\n",
        "\n",
        "# Define the function to create a TFRecord dataset from a list of TFRecord files\n",
        "def create_dataset(file_list):\n",
        "    dataset = tf.data.TFRecordDataset(file_list)\n",
        "    dataset = dataset.map(parse_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.shuffle(buffer_size=1000)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "9arNvm2v_iPf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the paths to the train and validation TFRecord files\n",
        "train_files = ['train.record']\n",
        "val_files = ['val.record']\n",
        "\n",
        "# Create the train and validation datasets\n",
        "batch_size = 16\n",
        "train_dataset = create_dataset(train_files)\n",
        "val_dataset = create_dataset(val_files)\n",
        "\n",
        "# Define the callbacks for the model training\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint('model.h5', save_best_only=True, save_weights_only=True)\n",
        "early_stopping_callback = tf.keras.callbacks.EarlyStopping(patience=10)\n",
        "\n",
        "# Train the model\n",
        "epochs = 100\n",
        "steps_per_epoch = 1000\n",
        "validation_steps = 100\n",
        "model.fit(train_dataset, epochs=epochs, steps_per_epoch=steps_per_epoch, validation_data=val_dataset, validation_steps=validation_steps, callbacks=[checkpoint_callback, early_stopping_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "id": "aQaE-3hj_sGX",
        "outputId": "f770a647-b6d9-43a1-a575-e7b037447722"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-3abce56d7c7c>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1047, in train_step\n        x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/data_adapter.py\", line 1775, in unpack_x_y_sample_weight\n        raise ValueError(error_msg)\n\n    ValueError: Data is expected to be in format `x`, `(x,)`, `(x, y)`, or `(x, y, sample_weight)`, found: (<tf.Tensor 'IteratorGetNext:0' shape=(None, None, None, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, None, 4) dtype=float32>, <tf.Tensor 'IteratorGetNext:2' shape=(None, None, 80) dtype=float32>, <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=float32>, <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=float32>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KgPIpkOn_13M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}